{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "load_dotenv(find_dotenv())\n",
    "sys.path.append(os.getenv(\"PROJECT_FOLDER\"))\n",
    "from src.utils.common import logger, read_yaml, create_directories\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    source_path: Path\n",
    "    cleaned_data_path: Path\n",
    "    transformed_data_path: Path\n",
    "    train_data_path: Path\n",
    "    test_data_path: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.common import read_yaml, create_directories\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath: str = os.getenv(\"CONFIG_FILE_PATH\"),\n",
    "        params_filepath: str = os.getenv(\"PARAMS_FILE_PATH\"),\n",
    "        schema_filepath: str = os.getenv(\"SCHEMA_FILE_PATH\"),\n",
    "    ):\n",
    "        self.config = read_yaml(Path(config_filepath))\n",
    "        self.params = read_yaml(Path(params_filepath))\n",
    "        self.schema = read_yaml(Path(schema_filepath))\n",
    "        \n",
    "        create_directories([self.config.artifacts_root])\n",
    "    \n",
    "    def get_data_transformation_config(self):\n",
    "        config = self.config.data_transformation\n",
    "        \n",
    "        create_directories([config.root_dir])\n",
    "        \n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            source_path=config.source_path,\n",
    "            cleaned_data_path=config.cleaned_data_path,\n",
    "            transformed_data_path=config.transformed_data_path,\n",
    "            train_data_path=config.train_data_path,\n",
    "            test_data_path=config.test_data_path,\n",
    "        )\n",
    "        \n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        \"\"\"\n",
    "        Instantiate `DataTransformation` class\n",
    "\n",
    "        Args:\n",
    "            config (DataTransformationConfig): configuration for data ingestion\n",
    "        \"\"\"\n",
    "        self.stopwords_en = stopwords.words(\"english\")\n",
    "        self.punctuations = string.punctuation\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.config = config\n",
    "    \n",
    "    def clean_data(self):\n",
    "        logger.info(f\"Clean Data\")\n",
    "        df = pd.read_csv(self.config.source_path)\n",
    "        df = df.drop_duplicates()  # drop duplicates\n",
    "        df = df.dropna(subset=['reviewText'], axis=0)  # drop missing `reviewText` columns\n",
    "        df = df[[\"reviewText\", \"sentiment\"]]  # select columns\n",
    "        df = df.reset_index(drop=True)  # reset index\n",
    "        df.to_csv(self.config.cleaned_data_path, index=False)\n",
    "\n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        tokens = word_tokenize(text.lower())  # normalize, remove punctuations, and tokenize text\n",
    "        filtered_tokens = [token for token in tokens if token not in self.stopwords_en and token not in self.punctuations]  # filter stop words\n",
    "        lemmatized_tokens = [self.lemmatizer.lemmatize(token) for token in filtered_tokens]  # lemmatize words\n",
    "        return \" \".join(lemmatized_tokens)  # Join the tokens back into a string\n",
    "    \n",
    "    def preprocess_texts(self):    \n",
    "        df = pd.read_csv(self.config.cleaned_data_path)\n",
    "        logger.info(f\"Preprocess text data\")\n",
    "        df[\"preprocessed_review_text\"] = df[\"reviewText\"].apply(self.preprocess_text)  # text preprocessing\n",
    "        df = df[(df[\"preprocessed_review_text\"].apply(lambda x: len(x)) != 0)]  # remove 0 length preprocess text\n",
    "        df = df[[\"preprocessed_review_text\", \"sentiment\"]]  # select columns for model training\n",
    "        df.to_csv(self.config.transformed_data_path, index=False)\n",
    "    \n",
    "    def split_data(self):    \n",
    "        df = pd.read_csv(self.config.transformed_data_path)\n",
    "        logger.info(f\"Split data\")\n",
    "        X, y = df[['preprocessed_review_text']], df[['sentiment']]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, shuffle=True, random_state=42)\n",
    "        \n",
    "        train = pd.concat([X_train, y_train], axis=1)\n",
    "        test = pd.concat([X_test, y_test], axis=1)\n",
    "        \n",
    "        train.to_csv(self.config.train_data_path, index=False)\n",
    "        test.to_csv(self.config.test_data_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-07 22:03:29,235 - sentiment-classifier-logger - INFO - yaml file: C:\\Users\\USER\\Documents\\GitHub\\customer-product-reviews-sentiment-classifier\\config\\config.yaml loaded successfully\n",
      "2024-03-07 22:03:29,235 - sentiment-classifier-logger - INFO - yaml file: C:\\Users\\USER\\Documents\\GitHub\\customer-product-reviews-sentiment-classifier\\params.yaml loaded successfully\n",
      "2024-03-07 22:03:29,243 - sentiment-classifier-logger - INFO - yaml file: C:\\Users\\USER\\Documents\\GitHub\\customer-product-reviews-sentiment-classifier\\schema.yaml loaded successfully\n",
      "2024-03-07 22:03:29,244 - sentiment-classifier-logger - INFO - Created directory at: artifacts\n",
      "2024-03-07 22:03:29,246 - sentiment-classifier-logger - INFO - Created directory at: artifacts/data_transformation\n",
      "2024-03-07 22:03:29,248 - sentiment-classifier-logger - INFO - Clean Data\n",
      "2024-03-07 22:03:29,630 - sentiment-classifier-logger - INFO - Preprocess text data\n",
      "2024-03-07 22:03:45,679 - sentiment-classifier-logger - INFO - Split data\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    configuration_manager = ConfigurationManager()\n",
    "    data_transformation = DataTransformation(config=configuration_manager.get_data_transformation_config())\n",
    "    data_transformation.clean_data()\n",
    "    data_transformation.preprocess_texts()\n",
    "    data_transformation.split_data()\n",
    "except Exception as e:\n",
    "    logger.error(e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-tools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
